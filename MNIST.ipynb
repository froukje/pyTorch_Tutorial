{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision #collection of datasets connected to visiontasks\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data\n",
    "train = datasets.MNIST(\"Data/\", #where the data should be saved\n",
    "                       train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"Data\", #where the data should be saved\n",
    "                       train=False, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and testset\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset  = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    #build the model\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    #define how the data goes through the layers    \n",
    "    def forward(self, x):\n",
    "        net.train() #set model in train mode\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([7, 9, 4, 1, 4, 7, 3, 3, 1, 8])]\n"
     ]
    }
   ],
   "source": [
    "# access the data\n",
    "for data in trainset:\n",
    "    print(data) #print 1 batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8526e-01, 7.6525e-01, 7.8407e-01, 7.2664e-01, 8.3212e-02, 2.7095e-01,\n",
       "         9.2851e-01, 2.8985e-01, 6.1082e-01, 5.0362e-01, 3.6588e-01, 5.5066e-01,\n",
       "         8.1170e-01, 8.6748e-01, 9.2109e-01, 8.7550e-01, 6.1876e-01, 9.9679e-01,\n",
       "         9.2661e-02, 9.6677e-01, 8.7147e-02, 9.8003e-01, 5.0975e-01, 4.3203e-01,\n",
       "         2.0241e-01, 1.9908e-01, 6.4430e-01, 1.6923e-01],\n",
       "        [7.5950e-01, 6.8897e-02, 2.1836e-02, 1.9324e-01, 4.5902e-01, 5.3658e-01,\n",
       "         1.7522e-02, 3.5621e-01, 3.8213e-04, 6.9458e-01, 3.6396e-01, 9.4305e-02,\n",
       "         7.7132e-01, 4.3581e-01, 7.5477e-01, 1.2753e-01, 9.6899e-01, 4.3463e-01,\n",
       "         9.2262e-01, 7.4896e-01, 7.5678e-02, 9.1095e-01, 1.0487e-01, 5.9205e-01,\n",
       "         9.4376e-01, 9.0935e-01, 6.2317e-01, 6.2413e-02],\n",
       "        [7.4472e-01, 4.0416e-01, 2.8917e-01, 8.6405e-01, 5.6232e-01, 6.2617e-01,\n",
       "         7.1003e-01, 4.8734e-01, 1.5631e-01, 5.6264e-01, 2.8577e-01, 3.8455e-01,\n",
       "         9.5689e-01, 2.1893e-01, 2.1139e-01, 9.8932e-01, 3.9691e-01, 4.0575e-01,\n",
       "         2.2228e-02, 7.6763e-01, 2.4966e-01, 1.6964e-01, 8.5394e-01, 9.4820e-01,\n",
       "         6.8777e-01, 9.2352e-01, 6.2496e-01, 2.8204e-01],\n",
       "        [8.8660e-01, 7.5903e-01, 9.6916e-01, 4.3408e-01, 5.9100e-01, 3.7817e-01,\n",
       "         6.1481e-01, 6.3246e-01, 4.5451e-01, 2.5013e-01, 3.2611e-01, 8.1813e-01,\n",
       "         7.6063e-01, 4.3123e-01, 5.2424e-01, 5.4947e-01, 6.8730e-01, 8.1171e-02,\n",
       "         2.2621e-01, 2.1799e-01, 3.9098e-01, 2.0145e-01, 5.4958e-01, 6.3522e-01,\n",
       "         4.3360e-01, 8.3161e-01, 8.7804e-01, 6.3906e-01],\n",
       "        [3.2056e-01, 7.7569e-01, 6.8887e-01, 3.7425e-01, 6.8153e-01, 5.2079e-01,\n",
       "         3.1388e-01, 5.9321e-02, 9.6744e-01, 9.7799e-02, 2.5816e-01, 5.9437e-01,\n",
       "         8.2212e-02, 5.6317e-01, 4.7837e-01, 2.8106e-01, 1.7798e-01, 6.7747e-01,\n",
       "         5.8289e-01, 8.0133e-01, 5.4164e-02, 1.8875e-01, 1.0314e-01, 6.7174e-01,\n",
       "         3.7674e-02, 7.6475e-01, 7.1145e-01, 8.6628e-01],\n",
       "        [4.6432e-01, 4.7803e-01, 2.6171e-01, 5.3329e-01, 6.7247e-01, 7.3990e-01,\n",
       "         2.4890e-01, 3.2033e-01, 2.0334e-01, 7.5710e-01, 5.7402e-01, 8.2590e-01,\n",
       "         3.8597e-01, 1.1635e-01, 5.7236e-01, 3.6310e-01, 3.9674e-01, 7.3081e-01,\n",
       "         2.7907e-01, 5.4581e-01, 9.7032e-01, 6.7132e-01, 5.4323e-01, 3.5387e-01,\n",
       "         3.5514e-01, 3.0262e-02, 1.4638e-02, 4.7298e-01],\n",
       "        [1.6241e-01, 6.7866e-01, 9.1430e-01, 1.0286e-02, 8.2889e-02, 4.1457e-01,\n",
       "         7.0403e-01, 1.3914e-01, 1.2055e-01, 5.0050e-01, 5.3633e-01, 9.4152e-01,\n",
       "         4.1115e-01, 1.3971e-01, 2.7618e-01, 1.3158e-01, 3.5658e-01, 7.9851e-01,\n",
       "         9.2001e-02, 8.6796e-01, 8.6969e-01, 2.9077e-01, 7.9292e-01, 1.8400e-01,\n",
       "         8.8001e-01, 4.4533e-01, 3.8827e-01, 1.3010e-01],\n",
       "        [9.0183e-01, 6.2264e-01, 8.4580e-01, 3.3280e-01, 8.6795e-01, 5.8866e-01,\n",
       "         2.3124e-01, 9.4146e-01, 4.8917e-01, 5.9232e-01, 2.9992e-01, 2.0050e-01,\n",
       "         5.3311e-01, 8.5170e-01, 5.6025e-01, 2.5106e-01, 3.3986e-01, 5.2194e-02,\n",
       "         4.4237e-01, 7.0824e-01, 7.7022e-02, 4.5178e-01, 4.2440e-01, 2.8231e-01,\n",
       "         6.5827e-01, 5.4162e-02, 7.7778e-01, 6.3885e-01],\n",
       "        [4.7897e-01, 1.6564e-01, 4.0236e-01, 3.4533e-01, 8.2784e-01, 4.5604e-01,\n",
       "         2.4299e-01, 3.1302e-01, 8.0423e-01, 1.8172e-01, 2.6444e-01, 2.0181e-01,\n",
       "         3.1401e-02, 1.4233e-02, 7.0697e-01, 2.6921e-01, 8.5199e-01, 8.4923e-01,\n",
       "         4.2140e-01, 2.5400e-01, 6.4850e-01, 7.8134e-01, 3.8516e-01, 7.5146e-01,\n",
       "         7.8507e-01, 7.9357e-01, 9.2562e-01, 8.4652e-01],\n",
       "        [7.3506e-01, 4.4815e-01, 4.2827e-01, 2.4964e-01, 6.9652e-01, 6.0534e-01,\n",
       "         1.5801e-03, 5.9270e-01, 3.6834e-02, 1.8581e-01, 4.1671e-01, 3.9393e-02,\n",
       "         4.3497e-01, 5.4043e-01, 3.4230e-02, 3.2472e-01, 5.0309e-01, 8.9713e-01,\n",
       "         7.1453e-01, 9.5582e-01, 1.2434e-04, 4.6915e-02, 1.4415e-02, 3.4250e-01,\n",
       "         8.3360e-01, 7.1438e-01, 2.7087e-01, 2.0745e-01],\n",
       "        [5.8753e-01, 4.8539e-01, 1.5810e-01, 3.3647e-01, 3.4725e-01, 3.7755e-01,\n",
       "         6.1920e-02, 4.3018e-01, 4.3645e-02, 8.3402e-01, 7.7232e-01, 1.7367e-01,\n",
       "         1.6201e-01, 1.4936e-01, 8.6708e-01, 3.1998e-01, 4.6612e-01, 4.3085e-01,\n",
       "         3.3687e-01, 4.8218e-01, 7.4653e-01, 5.2403e-01, 9.6295e-01, 2.6818e-01,\n",
       "         7.0607e-01, 7.6516e-01, 9.5419e-01, 9.1917e-02],\n",
       "        [5.6621e-01, 1.7367e-01, 4.4726e-01, 1.3582e-01, 5.9451e-01, 3.9012e-01,\n",
       "         5.4010e-01, 1.9245e-01, 4.5585e-01, 4.1887e-01, 3.1569e-01, 9.0471e-02,\n",
       "         1.4178e-01, 7.3799e-01, 4.6134e-01, 2.4811e-01, 1.4201e-01, 2.3944e-01,\n",
       "         3.1442e-01, 5.8630e-02, 7.1196e-02, 3.1104e-01, 2.6200e-01, 2.0172e-02,\n",
       "         4.3403e-01, 8.1727e-01, 4.8390e-01, 1.7142e-01],\n",
       "        [3.3564e-01, 2.3260e-01, 9.8574e-01, 8.1278e-01, 2.6744e-02, 5.7065e-01,\n",
       "         8.4542e-01, 5.5540e-01, 8.2743e-01, 2.2828e-02, 2.3599e-01, 1.8908e-01,\n",
       "         1.5408e-01, 5.8087e-01, 3.6872e-01, 4.8214e-01, 6.4904e-01, 1.2921e-02,\n",
       "         7.5047e-01, 2.6870e-01, 7.3540e-01, 1.0390e-01, 9.9932e-01, 8.9081e-01,\n",
       "         5.2577e-01, 7.4209e-01, 6.4426e-01, 3.8588e-02],\n",
       "        [7.7988e-01, 8.2260e-01, 4.5205e-01, 8.7732e-01, 9.7106e-01, 8.7635e-01,\n",
       "         8.6793e-01, 9.8281e-01, 3.2366e-01, 3.9371e-01, 4.6620e-01, 9.5177e-01,\n",
       "         4.0260e-01, 5.9295e-01, 9.8782e-02, 5.9353e-01, 3.1766e-01, 8.9473e-01,\n",
       "         7.2339e-01, 1.8870e-01, 7.9557e-01, 4.6409e-01, 3.3071e-02, 6.3663e-01,\n",
       "         6.3131e-01, 4.4899e-01, 2.3441e-01, 6.8552e-01],\n",
       "        [5.3483e-01, 9.2569e-01, 4.6475e-01, 2.0152e-01, 6.3636e-01, 1.9203e-01,\n",
       "         5.6069e-01, 2.2040e-01, 9.9136e-01, 2.9223e-01, 9.7824e-01, 4.2188e-01,\n",
       "         2.7878e-01, 1.8564e-01, 8.0446e-01, 7.4991e-01, 8.3589e-01, 6.4264e-01,\n",
       "         6.7288e-01, 8.3738e-01, 7.5169e-01, 5.3667e-01, 7.7264e-01, 5.6414e-01,\n",
       "         7.7281e-02, 5.8316e-01, 1.5188e-01, 8.8815e-01],\n",
       "        [3.0663e-01, 8.6676e-01, 4.5280e-01, 2.4841e-02, 7.4827e-01, 9.8596e-01,\n",
       "         6.7489e-01, 3.2464e-02, 4.4148e-01, 7.7140e-01, 4.4052e-01, 1.8035e-01,\n",
       "         4.6893e-01, 6.4382e-01, 8.5930e-01, 3.8563e-01, 9.2981e-01, 8.6782e-01,\n",
       "         1.3025e-01, 3.9198e-02, 8.5463e-01, 1.6264e-01, 1.9755e-02, 9.9631e-01,\n",
       "         8.5110e-01, 2.6727e-01, 4.4996e-01, 3.8699e-01],\n",
       "        [1.9875e-01, 1.1389e-01, 9.0079e-01, 8.0440e-01, 6.4535e-01, 8.0829e-01,\n",
       "         6.5195e-01, 2.6894e-01, 8.7141e-01, 2.0632e-01, 2.8572e-01, 8.1337e-01,\n",
       "         1.7193e-01, 8.0195e-02, 3.8880e-01, 8.7616e-01, 3.5970e-01, 6.9754e-01,\n",
       "         4.2924e-01, 3.7133e-01, 9.4846e-01, 8.8914e-03, 7.5594e-01, 6.6715e-01,\n",
       "         2.0579e-02, 1.0757e-01, 9.4200e-01, 4.7203e-01],\n",
       "        [8.3232e-01, 7.7499e-01, 2.5724e-01, 7.5509e-01, 7.6775e-01, 3.4519e-01,\n",
       "         2.0169e-01, 1.6663e-01, 6.4334e-01, 2.3372e-01, 6.0887e-01, 8.2727e-01,\n",
       "         9.2713e-01, 3.3401e-01, 4.7488e-01, 1.7446e-01, 1.1805e-01, 6.7883e-01,\n",
       "         7.3806e-01, 8.9755e-01, 4.3677e-01, 2.3034e-02, 9.7271e-01, 9.2737e-01,\n",
       "         9.6453e-01, 8.6536e-01, 2.5870e-01, 7.4681e-01],\n",
       "        [1.9142e-01, 5.5531e-01, 3.4873e-02, 5.9528e-01, 4.3437e-01, 2.5834e-01,\n",
       "         9.2774e-02, 1.8996e-01, 9.8622e-01, 8.8312e-01, 6.9088e-01, 3.7276e-01,\n",
       "         6.2321e-01, 3.1430e-01, 4.6463e-01, 3.5800e-01, 4.0793e-01, 8.4295e-01,\n",
       "         2.1252e-02, 2.4636e-01, 1.0735e-01, 3.2360e-02, 4.4748e-01, 2.3219e-01,\n",
       "         5.9896e-01, 6.5891e-01, 7.6375e-01, 6.2344e-01],\n",
       "        [5.7699e-01, 3.6544e-01, 4.7306e-02, 7.0202e-01, 3.1605e-01, 7.6748e-01,\n",
       "         4.2148e-01, 2.6008e-02, 7.1735e-01, 1.5448e-02, 3.7183e-01, 6.1645e-01,\n",
       "         6.9450e-01, 4.7871e-01, 7.7298e-01, 9.6223e-01, 9.0463e-01, 6.3329e-01,\n",
       "         2.7654e-01, 4.0772e-01, 7.1258e-01, 2.0675e-01, 2.1256e-01, 6.8068e-01,\n",
       "         9.6735e-01, 7.1271e-01, 9.0509e-01, 7.5492e-01],\n",
       "        [6.0399e-01, 6.9656e-01, 5.5243e-01, 9.0025e-01, 1.3839e-01, 1.2802e-01,\n",
       "         9.3547e-01, 2.5674e-01, 5.7326e-01, 1.4910e-01, 2.9514e-01, 8.0627e-01,\n",
       "         6.6482e-01, 7.8124e-01, 9.8017e-01, 8.5866e-01, 7.1502e-01, 1.6210e-01,\n",
       "         3.2926e-01, 5.8968e-01, 8.2163e-01, 7.5792e-01, 9.7618e-03, 2.9209e-01,\n",
       "         1.4490e-01, 4.4534e-01, 4.6090e-01, 1.3944e-01],\n",
       "        [4.5676e-01, 5.5135e-02, 8.1862e-01, 9.0491e-01, 5.5366e-01, 1.4831e-01,\n",
       "         8.2896e-01, 5.3359e-01, 5.2914e-01, 2.5714e-01, 8.8970e-01, 3.6721e-01,\n",
       "         2.6246e-01, 8.9531e-01, 3.3980e-01, 2.3074e-01, 3.6504e-01, 3.8559e-01,\n",
       "         8.9275e-01, 3.2869e-01, 6.5034e-01, 2.2168e-01, 2.9985e-01, 6.9357e-01,\n",
       "         7.9675e-01, 2.5415e-01, 9.8050e-01, 8.3078e-01],\n",
       "        [7.7417e-01, 1.9965e-01, 1.2894e-01, 1.5203e-01, 3.9026e-01, 3.8642e-01,\n",
       "         4.6598e-01, 7.6659e-01, 7.8529e-01, 1.3310e-01, 6.7596e-01, 9.3401e-01,\n",
       "         1.6180e-01, 4.2243e-01, 4.1815e-01, 2.0226e-01, 5.0774e-01, 3.6576e-01,\n",
       "         3.8721e-02, 5.5804e-01, 1.0525e-01, 8.9981e-02, 9.4010e-01, 6.6674e-01,\n",
       "         6.3936e-01, 6.3335e-01, 6.7221e-01, 4.7968e-01],\n",
       "        [3.7128e-01, 1.1122e-01, 9.4102e-01, 3.7626e-01, 5.8441e-01, 5.6907e-01,\n",
       "         5.4799e-01, 2.7866e-01, 6.1890e-01, 9.6857e-01, 9.2808e-01, 2.6188e-01,\n",
       "         4.3646e-01, 7.1011e-01, 2.8745e-01, 1.3338e-01, 7.3237e-01, 7.7614e-01,\n",
       "         4.2369e-01, 3.7365e-01, 8.5717e-02, 3.6318e-01, 9.7768e-01, 7.9476e-01,\n",
       "         2.1261e-01, 4.4909e-01, 2.4808e-01, 6.4565e-01],\n",
       "        [9.8311e-01, 6.4406e-01, 6.5925e-01, 5.0190e-01, 9.4061e-01, 2.2150e-01,\n",
       "         6.8174e-01, 2.2817e-01, 2.0771e-01, 7.1391e-01, 6.5495e-02, 4.3401e-01,\n",
       "         1.1299e-01, 7.8569e-01, 7.2364e-01, 9.8421e-01, 1.7253e-01, 4.4370e-01,\n",
       "         5.3387e-01, 3.4537e-02, 2.3653e-01, 3.2640e-01, 5.7911e-01, 7.5944e-01,\n",
       "         8.8988e-01, 9.3870e-01, 8.4743e-01, 8.4193e-02],\n",
       "        [2.5536e-01, 2.1074e-01, 1.8348e-01, 5.7103e-01, 1.1933e-01, 2.6021e-01,\n",
       "         3.4890e-01, 9.1140e-02, 9.1972e-01, 8.4464e-01, 9.7951e-03, 6.1802e-01,\n",
       "         7.1241e-01, 8.6531e-01, 3.3404e-01, 1.4991e-04, 6.7385e-01, 3.1019e-01,\n",
       "         8.0263e-01, 3.4186e-02, 8.2314e-01, 5.4477e-01, 5.7112e-01, 2.3645e-01,\n",
       "         4.9282e-02, 5.9102e-01, 6.0174e-01, 6.5446e-01],\n",
       "        [3.2057e-01, 3.8626e-01, 9.5384e-01, 7.3579e-01, 2.4188e-01, 7.5453e-01,\n",
       "         5.6425e-01, 7.8081e-01, 5.7303e-01, 8.2236e-01, 8.4035e-01, 5.4567e-01,\n",
       "         1.1306e-01, 7.4675e-02, 5.1439e-02, 2.3487e-02, 9.1165e-02, 1.3013e-03,\n",
       "         5.6694e-01, 9.5932e-01, 1.6518e-01, 5.5522e-01, 8.4696e-01, 1.7732e-02,\n",
       "         6.4479e-01, 2.4197e-02, 5.8240e-01, 6.8170e-01],\n",
       "        [5.0833e-01, 6.2429e-01, 9.1764e-01, 4.6247e-01, 7.4012e-01, 7.1218e-01,\n",
       "         1.7959e-01, 4.8142e-01, 3.1671e-01, 5.8775e-01, 4.9332e-01, 1.5978e-01,\n",
       "         1.4504e-01, 7.2327e-01, 5.2434e-01, 1.9938e-01, 2.8030e-01, 8.6288e-02,\n",
       "         2.2015e-01, 4.3148e-01, 1.7013e-02, 3.6678e-01, 9.1500e-01, 6.4142e-01,\n",
       "         4.9035e-01, 2.5766e-02, 9.2640e-01, 7.0516e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define some random data\n",
    "X_r = torch.rand([28, 28])\n",
    "X_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2600, -2.4476, -2.3856, -2.2222, -2.2660, -2.2054, -2.3134, -2.2695,\n",
       "         -2.4651, -2.2300]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the first image through the data\n",
    "output = net(X.view(-1,28*28))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2440, -2.4660, -2.4009, -2.2303, -2.2747, -2.2120, -2.3093, -2.2597,\n",
       "         -2.4443, -2.2247]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass the random data through the data\n",
    "output = net(X_r.view(-1,28*28))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0357, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0146, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# define an optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=.001)\n",
    "\n",
    "EPOCHS = 3 #3 passes through the entire dataset\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset: #data is a batch of features and labels \n",
    "        X, y = data\n",
    "        net.zero_grad() #before calculating loss, if not set to 0, gradients for different batches are added\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step() #adjusts the weights\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.978\n"
     ]
    }
   ],
   "source": [
    "# evaluation (on trainset)\n",
    "correct = 0\n",
    "total   = 0\n",
    "\n",
    "with torch.no_grad(): #gradients are not calculated\n",
    "    net.eval() #set model in eavaluation mode (i.e. ignoring dropouts and batchnorm)\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.968\n"
     ]
    }
   ],
   "source": [
    "# evaluation (on testset)\n",
    "correct = 0\n",
    "total   = 0\n",
    "\n",
    "with torch.no_grad(): #gradients are not calculated\n",
    "    net.eval() #set model in eavaluation mode (i.e. ignoring dropouts and batchnorm)\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0].view(28, 28));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(net(X[0].view(-1,28*28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
